{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0c973c0f3b75d278258c98f27af521ba2a5a3f33703eb568891dedf9795a850f8",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import missingno as msno\n"
   ]
  },
  {
   "source": [
    "## Imputation Part II"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_data = pd.read_csv('intermediate_df.csv')\n",
    "print(inter_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_data.info()"
   ]
  },
  {
   "source": [
    "There are still some NaNs in the Salinity, SiO3, ChlorA, O2 and Phaeop columns. While the aggregate distributions for some of these quantities might look a little complicated we know that each of these datasets are taken in consecutive bottle measurements on a single cast as a function of depth. \n",
    "\n",
    "For some of this data then it should be possible to interpolate. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inter_data['Depthm'], inter_data['Salnty'])\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "OK, anything above 1000 m is going to be imputed to the mean of the Salinity above 1000 m."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive_nan_visualiser(data, colname):\n",
    "    cumNaNs= data[colname].isna().astype(int).groupby(data[colname].notna().astype(int).cumsum()).sum()\n",
    "    plt.plot(cumNaNs)\n",
    "    plt.xlabel('Series Index')\n",
    "    plt.ylabel('# of consecutive NaNs')\n",
    "    plt.title(colname + ' Consecutive NaN visualization')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_nan_visualiser(inter_data, 'Salnty')"
   ]
  },
  {
   "source": [
    "Hmmm...there are some small nan clumps and then there are a few large nan clumps. Imputing on these large clumps by simple interpolation is not going to work. However, lets impute the small clumps and then impute values at large depths after series leveling off with the large depth mean. The large depth cut off is deeplim, the nan clump size limit for interpolation is nan_limit."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "I know its a little janky...i'll come up with something later that automatically estimates depth at which series levels off so that deeplim is not a free parameter."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def impute_by_depth(data, colname, deeplim = 1500, nan_limit = 3, depthname = 'Depthm' ):\n",
    "    comb_df = pd.concat([data[depthname], data[colname]], axis = 1)\n",
    "    \n",
    "\n",
    "    print(comb_df[colname].isna().sum())\n",
    "\n",
    "    comb_df.interpolate(method = 'linear', limit = nan_limit, inplace = True)\n",
    "    lowdepthval = comb_df[comb_df[depthname] >= deeplim].loc[:,colname].mean()\n",
    "    comb_df.loc[(comb_df[depthname] >= deeplim) & (comb_df[colname].isna() == True), colname] = lowdepthval\n",
    "    print(comb_df[colname].isna().sum())\n",
    "    return comb_df[colname]"
   ]
  },
  {
   "source": [
    "Alright, lets impute using the impute_by_depth function for chlorophyll, phaeopigments, and salinity. We will do no further imputation on these columns as they do not have obvious single valued relationships with other quantities, and because at low depth the variance on these quantities is really high. And its not obvious that imputing with a mean -- even one conditioned on depth -- is a good idea. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_impute = ['Salnty', 'ChlorA', 'Phaeop']\n",
    "imputedcols = pd.concat([impute_by_depth(inter_data, col, deeplim = 500, nan_limit = 3 ) for col in cols_to_impute], axis = 1)\n",
    "inter_data[cols_to_impute] =  imputedcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_data.info()"
   ]
  },
  {
   "source": [
    "The final thing to impute is the silicate column. From our first notebook, we do see some kind of a functional dependence between silicates vs. depth. We also have no NaNs in depth, so we can you use this for a function-based imputation. Note: there are some obvious interesting branchoffs from the main curve"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inter_data['Depthm'], inter_data['SiO3uM'])\n",
    "plt.xlabel('Depthm')\n",
    "plt.ylabel('SiO3um')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def tanh_coeff(x, a, b, c):\n",
    "    return a * np.tanh(b * x) + c\n",
    "\n",
    "nonandat = inter_data[['Depthm', 'SiO3uM']].dropna()\n",
    "xdata = nonandat['Depthm']\n",
    "ydata =  nonandat['SiO3uM']\n",
    "popt, pcov = curve_fit(tanh_coeff, xdata , ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inter_data['Depthm'], inter_data['SiO3uM'], label = 'data')\n",
    "plt.scatter(xdata, tanh_coeff(xdata, *popt), c = 'r', label = 'fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Good, we can use the tanh_coeff to impute. Provided that most of the NaNs dont live on the offshoots of the main tanh function, we should be pretty good."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SiO3vsdepth_impute = inter_data.loc[inter_data['SiO3uM'].isna() == True, ['Depthm','SiO3uM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_SiO3 = tanh_coeff(SiO3vsdepth_impute['Depthm'], *popt)\n",
    "print(imputed_SiO3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_data.loc[inter_data['SiO3uM'].isna() == True, 'SiO3uM'] = imputed_SiO3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_data['SiO3uM'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(inter_data['Depthm'], inter_data['SiO3uM'])\n",
    "plt.xlabel('Depthm')\n",
    "plt.ylabel('SiO3um')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "OK, we can see the imputed data in the high depth region. Nice. This worked. We are now finished with data imputation. Any remaining NaNs in the dataframe will be dropped as we have no hope of imputing these in a way that might screw too much with the data distributions."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = inter_data.dropna()\n",
    "final_data.info()"
   ]
  },
  {
   "source": [
    "Now we save this data to csv."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}