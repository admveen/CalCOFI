{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0c973c0f3b75d278258c98f27af521ba2a5a3f33703eb568891dedf9795a850f8",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load standard packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "import re\n",
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('pgdatastyle.mplstyle') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df = pd.read_csv('cast.csv', low_memory = False, delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df.head()"
   ]
  },
  {
   "source": [
    "#### The above file contains metadata about a given cast (cast station location in long/lat coordinates, date/time of cast, ship info, general ambient weather conditions). The cast ID (Cst_Cnt) is referenced in the bottle data and provides a way of linking this metadata to the physical measurements in the bottle data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle_df = pd.read_csv('bottle.csv', low_memory= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle_df.columns"
   ]
  },
  {
   "source": [
    "#### This is pretty nice. There's a lot of data here with measurements of various quantities. Information on some of these quantities is listed below:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data Definitions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "| Measurement | Explanation | \n",
    "| --- | --- | \n",
    "| Salinity (Salnty) | Measured in grams of salt per 1000 mL seawater. Often reported in parts per thousand.  |\n",
    "| Water Temperature (T_degC) | The local temperature (at a specific depth) of the seawater in Celsius |\n",
    "| Oxygen Level (O2ml_L) | The concentration of dissolved oxygen (ml/L) in seawater. \n",
    "| Phosphate Level (PO4uM) | Concentration of dissolved PO4 ion in seawater (units: micromol/L) |\n",
    "| Silicate Level (SiO3uM) | Concentration of silicate ion SiO3 in seawater (units: micromol/L) |\n",
    "| Nitrogen Dioxide Level (NO2uM) | Concentration of nitrogen dioxide in seawater (units: micromol/L) |\n",
    "| Ammonia Level (NH3uM) | Concentration of ammonia in seawater (units: micromol/L) | \n",
    "| Nitrate Level (NO3uM) | Concentration of nitrate ion NO3 in seawater (units: micromol/L) |\n",
    "| Chlorophyll A (ChlorA) | Concentration of Chlorophyll A in seawater (units: microgram/L) |\n",
    "| Phaeopigments(Phaeop) | Concentration of phaeopigments in seawater. (units: microgram/L) |\n",
    "| Sea level depth (Depthm) | Depth at which bottle measurement was taken (units: meter) |\n",
    "\n",
    "Other relevant definitions are:\n",
    "\n",
    "| Measurement | Explanation | \n",
    "| --- | --- |\n",
    "| Cast (Cst_Cnt) | Cast ID number. |\n",
    "| Btl_Cnt | Bottle identifier (for a given cast) for measurements taken as a function of depth.  |\n",
    "\n",
    "Other relevant columns will be the lat/longitude of the cast, and the datetime."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Massaging the metadata\n",
    "Let's inspect the cast data a little more closely and then clean it up and subset it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cast_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cast_df.describe().T)"
   ]
  },
  {
   "source": [
    "Alright, so we can see that the most important metadata (lat/long and datetimes) have no NaNs. There are very few NaNs in the Wind Direction / Wind Speed data. So we might want to keep these last two measurements. We'll also want the Cast Counts and Station IDs as these appear in both the Cast and Bottle measurement dataframes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df_subcols = ['Cst_Cnt', 'Sta_ID', 'Date', 'Time', 'Lat_Dec', 'Lon_Dec', 'Wind_Dir', 'Wind_Spd']\n",
    "cast_df = cast_df[cast_df_subcols]\n",
    "print(cast_df.head(10))"
   ]
  },
  {
   "source": [
    "#### Some obvious problems are the Time column (the generic date out front that doesnt make sense). There are also some NaNs in the time column that need to be dealth with. Also Date's and Time dtype is an object. We want to combine these columns into a single datetime"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df['Time'] = cast_df.loc[:,'Time'].str.replace('12/30/1899 ', '')\n",
    "print(cast_df['Time'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df['Time'].isna().sum()"
   ]
  },
  {
   "source": [
    "We had better deal with these NaNs. In general, hydrographic sampling doesn't depend too much on the time of day (otherwise, one might expect pretty frequent sampling at each station). While we kept the time of day (why throw it out?), we don't want to throw out data for which we don't have the time that the data was taken. For these we will just fill in with noon or '12:00:00'."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df['Time'].fillna('12:00:00', inplace = True)\n",
    "cast_df['Time'].isna().sum()"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df['DateTime'] = pd.to_datetime(cast_df['Date'] + ' ' + cast_df['Time'])\n",
    "cast_df = cast_df.drop(columns = ['Date', 'Time'])\n",
    "print(cast_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cast_df['DateTime'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df.isna().sum()"
   ]
  },
  {
   "source": [
    "Some NaNs in the Wind Spd and Wind Direction. Let's visualize this to see if we can/should do a simple impute on this data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].hist(cast_df['Wind_Spd'], bins = 15)\n",
    "ax[0].set_ylabel('Wnd_Spd')\n",
    "ax[1].hist(cast_df['Wind_Dir'], bins = 15)\n",
    "ax[1].set_ylabel('Wnd_Dir')\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "#### The median is probably a pretty OK place to impute on the Wind speed. But The Wind_Dir is strongly peaked 32-34 degrees."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cast_df['Wind_Spd'].median())\n",
    "cast_df['Wind_Spd'].fillna(cast_df['Wind_Spd'].median(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = cast_df['Wind_Dir'].mode().values[0]\n",
    "cast_df['Wind_Dir'].fillna(m, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df[['Wind_Spd','Wind_Dir']].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_df.isna().sum()"
   ]
  },
  {
   "source": [
    "So far so good. Let's use plotly to visualize the spatial distribution of the measurement stations where the cast were made. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_geo(cast_df, lat = 'Lat_Dec', lon = 'Lon_Dec')\n",
    "fig.update_geos(fitbounds = 'locations', resolution=50, showcoastlines=True, coastlinecolor=\"Red\", showland=True, landcolor=\"ForestGreen\", showocean=True, oceancolor=\"MidnightBlue\")\n",
    "fig.update_traces(marker = dict(size = 2, color = 'yellow'))\n",
    "fig.update_layout(title = 'CalCoFI Monitoring Locations', height=300, width = 500, margin={\"r\":10,\"t\":30,\"l\":10,\"b\":30})\n"
   ]
  },
  {
   "source": [
    "Now let's deal with the bottle_df now. We are going to subset the bottle data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a large number of columns\n",
    "bottle_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawmatcher = re.compile('^R_')\n",
    "rawcols = [colnames for colnames in bottle_df.columns if rawmatcher.search(colnames)]\n",
    "bottle_df = bottle_df.drop(columns = rawcols)\n",
    "print(bottle_df.columns)"
   ]
  },
  {
   "source": [
    "The 'R_quantity' columns are raw data columns that can be removed (i.e. CoCalFI has done some basic preprocessing for us already).The '_qual' columns or columns like 'NO2q' correspond to a data quality assessment. 0 or NaN on quality columns corresponds to good data. The scale goes to 9 where 9 are essentially sensor malfunction or omission.   "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle_df.isna().sum()"
   ]
  },
  {
   "source": [
    "OK. It's pretty obvious that there are a whole bunch of NaN dominated columns. Let's drop data that has more than 740K NaNs. These are NaN dominated columns and reflect measurements that have just recently started being taken by the CoCalFI survey, new labeling systems, etc."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_threshold = len(bottle_df) - 740000\n",
    "bottle_df = bottle_df.dropna(axis = 1, thresh = nan_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottle_df.columns"
   ]
  },
  {
   "source": [
    "#### All right, so we still have a bunch of columns left. Of the remaining, many of the columns are data quality/precision assessors which are useful for processing the raw data that we've already dropped. So let's get rid of these and any derived quantities like 'Oxy_umol/Kg', 'O2sat' (derived from O2ml_L). Subsetting on the columns to leave in we have:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['Cst_Cnt', 'Btl_Cnt', 'Sta_ID', 'Depthm', 'T_degC','Salnty', 'O2ml_L', 'STheta','ChlorA', 'Phaeop', 'PO4uM', 'SiO3uM', 'NO2uM', 'NO3uM']\n",
    "bottle_df = bottle_df[cols_to_keep]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = cast_df.merge(bottle_df, on = ['Cst_Cnt', 'Sta_ID'], how = 'inner')"
   ]
  },
  {
   "source": [
    "Let's visualize where the NaNs are in our dataframe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(merged_df, sparkline=False, figsize = (10,5), color=(0.27, 0.52, 1.0))"
   ]
  },
  {
   "source": [
    "Most of the mineral measurements (nitrates, silicates, phosphates) and photopigment concentrations are NaNs in the first part of the dataset. As the Cst_Cnt is time ordered, the logical conclusion is that most of these measurements were made available after a specific date (i.e. due to technological advances or because they started to become metrics of interest to the community)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's drop everything before the year where Chlorophyll/Phaepigment data is available and where the bulk of the mineral data is also available. This is a lot of data to drop, I know...but at least for now we will restrict the study to data where we can justifiably impute. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startindex = merged_df[['ChlorA','Phaeop']].first_valid_index()\n",
    "print(startindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_restricted = merged_df.loc[startindex::,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets visualize missing no now\n",
    "msno.matrix(merged_df_restricted)"
   ]
  },
  {
   "source": [
    "## EDA/Visualization for Imputation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### A lot of this data is likely correlated. Let's subset the part of the data that we immediately expect to be features (physical bottle measurements as opposed to index data / metadata)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = merged_df_restricted.columns.drop(['Cst_Cnt', 'Sta_ID', 'Lat_Dec', 'Lon_Dec', 'Wind_Dir', 'Wind_Spd',\n",
    "       'DateTime', 'Btl_Cnt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.heatmap(merged_df_restricted[feat_names].corr())"
   ]
  },
  {
   "source": [
    "print(merged_df_restricted[feat_names].corr())"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "### Correlations \n",
    "#### Dissolved Oxygen\n",
    "The dissolved oxygen level (O2ml_L) is strongly anticorrelated with the phosphate (PO4), silicate (SiO3), and nitrate levels (NO3). These minerals -- particularly phosphates -- are often directly related to surface phytoplankton population growth. The strong negative correlation is rather interesting and merits futher investigation. \n",
    "\n",
    "The Pearson correlation coefficient also is pretty large (positive) between O2 and Temperature, large (negative) between O2 and Salinity/Depth/STheta (Stheta is the sigma theta which is the seawater density).\n",
    "\n",
    "#### Temperature\n",
    "The temperature has a strong negative correlation with the depth (the deeper you go the colder it gets...perhaps not too surprising). \n",
    "\n",
    "#### Salinity\n",
    "The Pearson correlation coefficient between depth and Salinity is positive (possibly indicating the deeper you go the more saline the water). This also seems to be aligned with the correlation between sigma-theta vs depth and sigma-theta vs. salinity.\n",
    "\n",
    "Almost all these variables are moderately to highly correlated with each other. However, coastal oceans are complex and these relations are likely not so simple. A quick visualization of the relationship of these variables across the entire dataset (as a first go) will be useful (not just for EDA but for possible imputation strategies)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(merged_df_restricted[feat_names].sample(10000)) #we downsample or else this would take forever"
   ]
  },
  {
   "source": [
    "There is a lot going on here. But here are my key take-aways:\n",
    "1. The phosphate and nitrate vs. dissolved O2 relationship is well captured by a linear relationship with a negative Pearson correlation coefficient. The silicate vs dissolved O2 largely seems to be as well, but there is a branch of the data that has a positive relationship. Probably worth thinking about why that might be.\n",
    "\n",
    "2. The phosphate and nitrate concentrations follow a pretty direct linear relationship. On the other hand, the phosphate and nitrate concentrations exhibit an increase for a large range of silicate concentraton and then maximum/decrease for high silicate concentrations. Very clearly, this is a nonlinear relationship (approximal linear in the low-mid silicate range). In fact, the bend in the data looks like its whats responsible for the second positively correlated branch in the SiO3 vs O2 data.\n",
    "\n",
    "\n",
    "3. The NO3, PO4,SiO3 vs Temp scatterplots clearly show why the Pearson correlation coefficients were highly negative. However, a preliminary look at these scatterplots on the downsampled data show that the relationship is more complex than in the minerals vs. dissolved O2 case.\n",
    "\n",
    "There is an initial increase in the NO3, PO4,SiO3 vs Temp scatterplots for low temperatures. For a large subset of the data theres a decreasing rolloff to zero. Some subset of the data follows a slower linear decrease to 0.\n",
    "\n",
    "4. NO2 doesnt really look that interesting. We should drop this column.\n",
    "\n",
    "5. Chlorophyll and phaeopigments are concentrated within the first 200 m of depth or so. That would make a lot of sense as this is the zone where light can actually reach.\n",
    "\n",
    "6. The salinity vs other variables (Temp, Stheta, O2, PO4, NO3, SiO3, depth ) shows rather clearly that there are two different subsets of the data following linear trends of opposing slopes. These might correspond directly to the two components of the Gaussian mixture distribution in the salinity histogram (something thats easy to check).\n",
    "\n",
    "7. The various variable vs depth is quite interesting:\n",
    "- O2 vs depth first has a decrease then a plumetting down to very low oxygen levels at intermediate depths and then a partial recovery at large depths.\n",
    "- Temperature vs depth shows a strong decline in intermediate depth regions and then a leveling out at high depths close to 0 C. There is also indcaions of temperature leveling out or slowing down its decline at intermediate depths -- there are clearly multiple types of behavior in this graph.\n",
    "- PO4 and NO3 rise to maxima at roughly the same depth as O2 gets its minimum then starts slowly decreasing as depth increases (mirroring the slow increase in O2 at large depths). \n",
    "- At low depths the salinity start at a higher level or a lower salinity level and then basically asymptote to the same value at higher depths.\n",
    "\n",
    "#### For O2, PO4, NO3 there is probably three different regions (region of higher O2 at surface depth, then low O2 levels in intermediate depths, and then the O2 recovery region)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now let's take a closer at the distribution for the various variables on the entire dataset:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('pgdatastyle.mplstyle')  \n",
    "for column in feat_names:        \n",
    "    plt.hist(merged_df_restricted[column], bins = 60, density=True)\n",
    "    plt.title('Distribution of ' + column)\n",
    "    plt.grid()\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Normalized Count')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "There's a lot going on here. It's pretty clear that the nitrate, silicate, phosphate, temperature, and oxygen distributions are indeed some kind of mixture of distributions. The matching structure of these mixture sub-distributions reinforce the high Pearson correlations between oxygen level and nitrates, silicates, and phosphates and provide stronger evidence of the linkage of these quantities. Also, the fact this looks like roughly three components to the mixture of these distributions, and our previous observations on depth dependence may lend credence to the hypothesis that there is roughly speaking at least a three-layer depth stratification of these quantities.\n",
    "\n",
    "The salinity looks like a bimodal distribution while the temperature is pretty difficult to describe in simple terms. \n",
    "\n",
    "Let's take a closer look at the chloropyll and phaeopigment distributions: \n",
    "\n",
    " "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(merged_df_restricted['ChlorA'], range = (0,1), bins = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='ChlorA', data = merged_df_restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(merged_df_restricted['Phaeop'], range = (0,1), bins = 200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y='Phaeop', data = merged_df_restricted)"
   ]
  },
  {
   "source": [
    "Both Chlorophyll and Phaeopigment concentrations look like there is a component that is Gamma-distributed. Chlorophyll has an additional component at low Chlorophyll values. There are portions of both distributions that are quite extreme and well outside the 1-3 IQR. What is going on with these values? They are likely not measurement errors but values where surface concentrations do indeed become quite extreme. There are a lot more NaNs in these columns. Is it possible that its because these measurements were usually only taken at surface levels (where they are likely non-zero)?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = merged_df_restricted[['Depthm', 'ChlorA', 'Phaeop']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['ChlorA'].isna() == True].hist(column = 'Depthm', bins = 60)\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['ChlorA'].notna() == True].hist(column = 'Depthm', bins = 50)\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'Depthm', y = 'ChlorA', data = df1)"
   ]
  },
  {
   "source": [
    "The moral of the above visualizations is that a lot of NaNs in ChlorA correspond to depths > 150 m while most of the recorded data was made at depth <= 200 m. But from the scatterplot most of the recorded Chlorophyll data is 0 anyway. So we'll impute NaNs at those lower depths to 0.\n",
    "\n",
    "Imputing NaNs within the <= 100 m depth range might be possible depending on the data. There is a high degree of variability in this data and no obvious strong correlations on other features in aggregate. So casts where all the chlorophyll data or most of the surface chlorophyll data is missing probably shouldnt be imputed. We'll just leave those datapoint as NaNs. But there is likely a well defined depth dependence in a single cast. It might be possible to interpolate or extrapolate based off other Chlorophyll data in the cast if these exist and our NaNs are single holes in the cast data record. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['Phaeop'].isna() == True].hist(column = 'Depthm', bins = 60)\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[df1['Phaeop'].notna() == True].hist(column = 'Depthm', bins = 50)\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x = 'Depthm', y = 'Phaeop', data = df1)"
   ]
  },
  {
   "source": [
    "Pretty much the same story as with chlorophyll. We'll take care of cleaning the chlorophyll snd phaeopigment column in a little bit."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_restricted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_restricted.loc[ ((merged_df_restricted['ChlorA'].isna() == True) | (merged_df_restricted['Phaeop'].isna() == True) ) & (merged_df_restricted['Depthm'] >=150), ['ChlorA', 'Phaeop']] = merged_df_restricted.loc[ ((merged_df_restricted['ChlorA'].isna() == True) | (merged_df_restricted['Phaeop'].isna() == True) ) & (merged_df_restricted['Depthm'] >=150), ['ChlorA', 'Phaeop']].replace({np.nan: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_restricted.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_restricted = merged_df_restricted.drop(columns = ['NO2uM']) # nothing super interesting is happening with NO2 so im dropping it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(merged_df_restricted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_restricted[merged_df_restricted['Phaeop'].isna() == True].hist(column = 'Depthm', bins = 5)\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_restricted[merged_df_restricted['ChlorA'].isna() == True].hist(column = 'Depthm', bins = 5)\n",
    "plt.xlim(0,1000)"
   ]
  },
  {
   "source": [
    "That looked like it worked."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "We've established strong correlations and nearly linear relationships between many of the other features in the dataset. We can take advantage of this for imputation purposes. We'll subset on these features and perform MICE using sklearn's iterative imputer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "source": [
    "We dropped silicates for imputation as its dependence on the other variables is not linear. We could do a feature transformation and then use MICE on the transformed set and transform back, but a simpler approach might be just as good for imputation purposes. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsubimp = ['O2ml_L', 'PO4uM', 'NO3uM'] \n",
    "data_imp_subset = merged_df_restricted[colsubimp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_imp_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = IterativeImputer(sample_posterior = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputeddata = imputer.fit_transform(data_imp_subset)"
   ]
  },
  {
   "source": [
    "We are leaving the imputed oxygen levels out as we tried this and it skewed the oxygen depth distribution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputeddf = pd.DataFrame(imputeddata, columns=['O2ml_L','PO4uM', 'NO3uM'], index = data_imp_subset.index)\n",
    "imputeddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_restricted[colsubimp].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_restricted[['PO4uM', 'NO3uM']] = imputeddf[['PO4uM', 'NO3uM']] #not putting in O2 imputed values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_restricted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_restricted.info()"
   ]
  },
  {
   "source": [
    "Still some NaNs in ChlorA, Phaeop, SiO3uM and Salnty. Before we deal with these, let's save our current dataframe to file. The iterative imputer with sampling the posterior took a little while and we've done a good amount of data transformations. Would be good to start from a saved csv file for the rest of the cleaning.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_restricted.to_csv('intermediate_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}